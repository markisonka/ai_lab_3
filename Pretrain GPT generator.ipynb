{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python39\\lib\\site-packages\\setuptools\\distutils_patch.py:25: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.\n",
      "  warnings.warn(\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm, trange\n",
    "import torch.nn.functional as F\n",
    "import csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка Датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Prepare data\n",
    "lyrics = pd.read_csv('dataHPMRM.csv')\n",
    "# lyrics = lyrics[lyrics['language']=='en']\n",
    "\n",
    "# #Only keep popular artists, with genre Rock/Pop and popularity high enough\n",
    "# artists = pd.read_csv('artists-data.csv')\n",
    "# artists = artists[(artists['Genres'].isin(['Rock'])) & (artists['Popularity']>5)]\n",
    "df = lyrics\n",
    "# df = df.drop(columns=['ALink','SLink','language','Link'])\n",
    "\n",
    "# #Drop the songs with lyrics too long (after more than 1024 tokens, does not work)\n",
    "df = df[df['text'].apply(lambda x: len(x.split(' ')) < 350)]\n",
    "\n",
    "# #Create a very small test set to compare generated text with the reality\n",
    "test_set = df.sample(n = 200)\n",
    "df = df.loc[~df.index.isin(test_set.index)]\n",
    "\n",
    "# #Reset the indexes\n",
    "test_set = test_set.reset_index()\n",
    "df = df.reset_index()\n",
    "\n",
    "# #For the test set only, keep last 20 words in a new column, then remove them from original column\n",
    "test_set['True_end_text'] = test_set['text'].str.split().str[-20:].apply(' '.join)\n",
    "test_set['text'] = test_set['text'].str.split().str[-20:].apply(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Предпоследний цветок колокольчика медленно погрузился в зелье.\n"
     ]
    }
   ],
   "source": [
    "print(test_set[\"text\"][0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка модели и Токенизатора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1219 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "class SongLyrics(Dataset):  \n",
    "    def __init__(self, control_code, truncate=False, gpt2_type=\"gpt2\", max_length=1024):\n",
    "\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(gpt2_type)\n",
    "        self.lyrics = []\n",
    "\n",
    "        for row in df['text']:\n",
    "          self.lyrics.append(torch.tensor(\n",
    "                self.tokenizer.encode(f\"<|{control_code}|>{row[:max_length]}<|endoftext|>\")\n",
    "            ))               \n",
    "        if truncate:\n",
    "            self.lyrics = self.lyrics[:20000]\n",
    "        self.lyrics_count = len(self.lyrics)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.lyrics_count\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.lyrics[item]\n",
    "    \n",
    "dataset = SongLyrics(df['text'], truncate=True, gpt2_type=\"gpt2\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для наглядности будем работать с русскоязычной GPT от Сбера.\n",
    "# Ниже команды для загрузки и инициализации модели и токенизатора.\n",
    "model_name_or_path = \"sberbank-ai/rugpt3large_based_on_gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name_or_path)\n",
    "\n",
    "#Accumulated batch size (since GPT2 is so big)\n",
    "def pack_tensor(new_tensor, packed_tensor, max_seq_len):\n",
    "    if packed_tensor is None:\n",
    "        return new_tensor, True, None\n",
    "    if new_tensor.size()[1] + packed_tensor.size()[1] > max_seq_len:\n",
    "        return packed_tensor, False, new_tensor\n",
    "    else:\n",
    "        packed_tensor = torch.cat([new_tensor, packed_tensor[:, 1:]], dim=1)\n",
    "        return packed_tensor, True, None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функция обучения модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    dataset, model, tokenizer,\n",
    "    batch_size=16, epochs=5, lr=2e-5,\n",
    "    max_seq_len=400, warmup_steps=200,\n",
    "    gpt2_type=\"gpt2\", output_dir=\".\", output_prefix=\"wreckgar\",\n",
    "    test_mode=False,save_model_on_epoch=False,\n",
    "):\n",
    "    acc_steps = 100\n",
    "    device=torch.device(\"cuda\")\n",
    "    model = model.cuda()\n",
    "    model.train()\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=-1\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "    loss=0\n",
    "    accumulating_batch_count = 0\n",
    "    input_tensor = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        print(f\"Training epoch {epoch}\")\n",
    "        print(loss)\n",
    "        for idx, entry in tqdm(enumerate(train_dataloader)):\n",
    "            (input_tensor, carry_on, remainder) = pack_tensor(entry, input_tensor, 768)\n",
    "\n",
    "            if carry_on and idx != len(train_dataloader) - 1:\n",
    "                continue\n",
    "\n",
    "            input_tensor = input_tensor.to(device)\n",
    "            outputs = model(input_tensor, labels=input_tensor)\n",
    "            loss = outputs[0]\n",
    "            loss.backward()\n",
    "\n",
    "            if (accumulating_batch_count % batch_size) == 0:\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                model.zero_grad()\n",
    "\n",
    "            accumulating_batch_count += 1\n",
    "            input_tensor = None\n",
    "        if save_model_on_epoch:\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                os.path.join(output_dir, f\"{output_prefix}-{epoch}.pt\"),\n",
    "            )\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модель генерации данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.73it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.09s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.79it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.85it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.02it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.18s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.98it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.23s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.15s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.22s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.74it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.83it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.18s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.76it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.89it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.28s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.18it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.29it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.97it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.89it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.06it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.70it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.17it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.20it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.17it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.32it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.22it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.28it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.13it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.78it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.22it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.47it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.88it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.22s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.30s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.94it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.85it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.28it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.15it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.51it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.72it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.25s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.80it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.64it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.95it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.15s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.10it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.96it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.23s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.09s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.75it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.19s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.17s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.21it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.21s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.24it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.85it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.27s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.01s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.26s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.14s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.03it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.16s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.55it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.36it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.44it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.50it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.41it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.23s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  9.61it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.57it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.60it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.62it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.30it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.40it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.53it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.22s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.96it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.85it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.41it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.44it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.24it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.82it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.32it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.19s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.53it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.16it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.01s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.15s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.13it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.60it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.08it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.23s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.27it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.38it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.25s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.39it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.33it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.23it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.25it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.93it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.50it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.67it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.73it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.27it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.27s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.64it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.11s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.35it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.25it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.11it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.18it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.17s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.70it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.06s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.28it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.35it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.23s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.11it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.20s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.27it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.06it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.01s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.22it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.09s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.95it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.41it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.26it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.41it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.10s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.73it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.19s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.33it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.17s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.55it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.46it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.20s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.12s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.34it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.28s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.30s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.70it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.28it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.32s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.97it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.62it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.73it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.18s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.06it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.24it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.25s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.01it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.87it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.01it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.05it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.57it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.54it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.55it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.22s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.18s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.01s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.09s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 27.01it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.61it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.17it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.21s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.34it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.69it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.61it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.74it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.40it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.86it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.24it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.18s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.26it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.26it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.14it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.39it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.01it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.06s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.05it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.70it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.71it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.82it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.90it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.88it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.11s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.08it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.62it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.55it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.20s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.90it/s]\n"
     ]
    }
   ],
   "source": [
    "def generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    entry_count=10,\n",
    "    entry_length=30, #maximum number of words\n",
    "    top_p=0.8,\n",
    "    temperature=1.,\n",
    "):\n",
    "    model.eval()\n",
    "    generated_num = 0\n",
    "    generated_list = []\n",
    "\n",
    "    filter_value = -float(\"Inf\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for entry_idx in trange(entry_count):\n",
    "\n",
    "            entry_finished = False\n",
    "            generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "\n",
    "            for i in range(entry_length):\n",
    "                generated = generated.to(\"cuda\")\n",
    "                outputs = model(generated, labels=generated)\n",
    "                loss, logits = outputs[:2]\n",
    "                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
    "\n",
    "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n",
    "                    ..., :-1\n",
    "                ].clone()\n",
    "                sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "                logits[:, indices_to_remove] = filter_value\n",
    "\n",
    "                next_token = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n",
    "                generated = torch.cat((generated, next_token), dim=1)\n",
    "\n",
    "                if next_token in tokenizer.encode(\".\"):\n",
    "                    entry_finished = True\n",
    "\n",
    "                if entry_finished:\n",
    "\n",
    "                    generated_num = generated_num + 1\n",
    "\n",
    "                    output_list = list(generated.to(\"cpu\").squeeze().numpy())\n",
    "                    output_text = tokenizer.decode(output_list)\n",
    "                    generated_list.append(output_text)\n",
    "                    break\n",
    "            \n",
    "            if not entry_finished:\n",
    "              output_list = list(generated.to(\"cpu\").squeeze().numpy())\n",
    "              output_text = f\"{tokenizer.decode(output_list)}.\" \n",
    "              generated_list.append(output_text)\n",
    "                \n",
    "    return generated_list\n",
    "\n",
    "#Function to generate multiple sentences. Test data should be a dataframe\n",
    "def text_generation(test_data):\n",
    "  generated_lyrics = []\n",
    "  for i in range(len(test_data)):\n",
    "    x = generate(model.to('cuda'), tokenizer, test_data['text'][i], entry_count=1)\n",
    "    generated_lyrics.append(x)\n",
    "  return generated_lyrics\n",
    "\n",
    "#Run the functions to generate the lyrics\n",
    "generated_lyrics = text_generation(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop to keep only generated text and add it as a new column in the dataframe\n",
    "my_generations=[]\n",
    "\n",
    "for i in range(len(generated_lyrics)):\n",
    "  a = test_set['text'][i].split()[-30:] #Get the matching string we want (30 words)\n",
    "  b = ' '.join(a)\n",
    "  c = ' '.join(generated_lyrics[i]) #Get all that comes after the matching string\n",
    "  my_generations.append(c.split(b)[-1])\n",
    "\n",
    "test_set['Generated_lyrics'] = my_generations\n",
    "\n",
    "\n",
    "#Finish the sentences when there is a point, remove after that\n",
    "final=[]\n",
    "\n",
    "for i in range(len(test_set)):\n",
    "  to_remove = test_set['Generated_lyrics'][i].split('.')[-1]\n",
    "  final.append(test_set['Generated_lyrics'][i].replace(to_remove,''))\n",
    "\n",
    "test_set['Generated_lyrics'] = final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>lable</th>\n",
       "      <th>text</th>\n",
       "      <th>True_end_text</th>\n",
       "      <th>Generated_lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21170</td>\n",
       "      <td>21171</td>\n",
       "      <td>1</td>\n",
       "      <td>Предпоследний цветок колокольчика медленно пог...</td>\n",
       "      <td>Предпоследний цветок колокольчика медленно пог...</td>\n",
       "      <td>\\n\\n– Итак, кто первый, Митра?\\n\\n– Чего ты хо...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15222</td>\n",
       "      <td>15222</td>\n",
       "      <td>1</td>\n",
       "      <td>профессор Зельеварения на самом деле, но, к со...</td>\n",
       "      <td>профессор Зельеварения на самом деле, но, к со...</td>\n",
       "      <td>\\n\\nОна была уверена, что её внешность была об...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11718</td>\n",
       "      <td>11718</td>\n",
       "      <td>1</td>\n",
       "      <td>— Ничуточки. Не будь я тебе обязана...</td>\n",
       "      <td>— Ничуточки. Не будь я тебе обязана...</td>\n",
       "      <td>\\n\\nНа ночь в ночь он прихватил с собой гитару...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11227</td>\n",
       "      <td>11227</td>\n",
       "      <td>1</td>\n",
       "      <td>— Спасибо, Белла, — ответил бесстрастный голос...</td>\n",
       "      <td>— Спасибо, Белла, — ответил бесстрастный голос...</td>\n",
       "      <td>— Он осторожно высвободил руку, которой, каза...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22079</td>\n",
       "      <td>22080</td>\n",
       "      <td>1</td>\n",
       "      <td>Хогвартса, с нашим профессором Квирреллом. Это...</td>\n",
       "      <td>Хогвартса, с нашим профессором Квирреллом. Это...</td>\n",
       "      <td>\\n\\nВспомнив это, Маклагген покачал головой.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>21418</td>\n",
       "      <td>21419</td>\n",
       "      <td>1</td>\n",
       "      <td>но не забывай об этом, ― в его руке снова оказ...</td>\n",
       "      <td>но не забывай об этом, ― в его руке снова оказ...</td>\n",
       "      <td>\\n\\nДжеки подчинился.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>7229</td>\n",
       "      <td>7229</td>\n",
       "      <td>1</td>\n",
       "      <td>— Что за бре...</td>\n",
       "      <td>— Что за бре...</td>\n",
       "      <td>— Так начиналась фраза, но вовремя оборвалась...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>20916</td>\n",
       "      <td>20917</td>\n",
       "      <td>1</td>\n",
       "      <td>что я говорил тебе раньше про заклинание крест...</td>\n",
       "      <td>что я говорил тебе раньше про заклинание крест...</td>\n",
       "      <td>Но не делай глупостей.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>1737</td>\n",
       "      <td>1737</td>\n",
       "      <td>1</td>\n",
       "      <td>этот провал обозначает потерю всего. Ты веришь...</td>\n",
       "      <td>этот провал обозначает потерю всего. Ты веришь...</td>\n",
       "      <td>Вместо того, чтобы признать правду и признать...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>20374</td>\n",
       "      <td>20375</td>\n",
       "      <td>1</td>\n",
       "      <td>— Подождите! — выпалил Гарри.</td>\n",
       "      <td>— Подождите! — выпалил Гарри.</td>\n",
       "      <td>— Я же вам говорил, что профессор Трелони его...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index  Unnamed: 0  lable  \\\n",
       "0    21170       21171      1   \n",
       "1    15222       15222      1   \n",
       "2    11718       11718      1   \n",
       "3    11227       11227      1   \n",
       "4    22079       22080      1   \n",
       "..     ...         ...    ...   \n",
       "195  21418       21419      1   \n",
       "196   7229        7229      1   \n",
       "197  20916       20917      1   \n",
       "198   1737        1737      1   \n",
       "199  20374       20375      1   \n",
       "\n",
       "                                                  text  \\\n",
       "0    Предпоследний цветок колокольчика медленно пог...   \n",
       "1    профессор Зельеварения на самом деле, но, к со...   \n",
       "2               — Ничуточки. Не будь я тебе обязана...   \n",
       "3    — Спасибо, Белла, — ответил бесстрастный голос...   \n",
       "4    Хогвартса, с нашим профессором Квирреллом. Это...   \n",
       "..                                                 ...   \n",
       "195  но не забывай об этом, ― в его руке снова оказ...   \n",
       "196                                    — Что за бре...   \n",
       "197  что я говорил тебе раньше про заклинание крест...   \n",
       "198  этот провал обозначает потерю всего. Ты веришь...   \n",
       "199                      — Подождите! — выпалил Гарри.   \n",
       "\n",
       "                                         True_end_text  \\\n",
       "0    Предпоследний цветок колокольчика медленно пог...   \n",
       "1    профессор Зельеварения на самом деле, но, к со...   \n",
       "2               — Ничуточки. Не будь я тебе обязана...   \n",
       "3    — Спасибо, Белла, — ответил бесстрастный голос...   \n",
       "4    Хогвартса, с нашим профессором Квирреллом. Это...   \n",
       "..                                                 ...   \n",
       "195  но не забывай об этом, ― в его руке снова оказ...   \n",
       "196                                    — Что за бре...   \n",
       "197  что я говорил тебе раньше про заклинание крест...   \n",
       "198  этот провал обозначает потерю всего. Ты веришь...   \n",
       "199                      — Подождите! — выпалил Гарри.   \n",
       "\n",
       "                                      Generated_lyrics  \n",
       "0    \\n\\n– Итак, кто первый, Митра?\\n\\n– Чего ты хо...  \n",
       "1    \\n\\nОна была уверена, что её внешность была об...  \n",
       "2    \\n\\nНа ночь в ночь он прихватил с собой гитару...  \n",
       "3     — Он осторожно высвободил руку, которой, каза...  \n",
       "4         \\n\\nВспомнив это, Маклагген покачал головой.  \n",
       "..                                                 ...  \n",
       "195                              \\n\\nДжеки подчинился.  \n",
       "196   — Так начиналась фраза, но вовремя оборвалась...  \n",
       "197                             Но не делай глупостей.  \n",
       "198   Вместо того, чтобы признать правду и признать...  \n",
       "199   — Я же вам говорил, что профессор Трелони его...  \n",
       "\n",
       "[200 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "– Итак, кто первый, Митра?\n",
      "\n",
      "– Чего ты хочешь, Великий.\n",
      "\n",
      "\n",
      "Она была уверена, что её внешность была обманчивой.\n",
      "\n",
      "\n",
      "На ночь в ночь он прихватил с собой гитару, а днем, когда солнце только-только начало припекать, вдруг совершил небольшой.\n",
      " — Он осторожно высвободил руку, которой, казалось, касался ее подбородок.\n",
      "\n",
      "\n",
      "Вспомнив это, Маклагген покачал головой.\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(test_set['Generated_lyrics'][i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2rc1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
