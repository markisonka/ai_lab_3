{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python39\\lib\\site-packages\\setuptools\\distutils_patch.py:25: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.\n",
      "  warnings.warn(\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm, trange\n",
    "import torch.nn.functional as F\n",
    "import csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Prepare data\n",
    "lyrics = pd.read_csv('dataHPMRM.csv')\n",
    "# lyrics = lyrics[lyrics['language']=='en']\n",
    "\n",
    "# #Only keep popular artists, with genre Rock/Pop and popularity high enough\n",
    "# artists = pd.read_csv('artists-data.csv')\n",
    "# artists = artists[(artists['Genres'].isin(['Rock'])) & (artists['Popularity']>5)]\n",
    "df = lyrics\n",
    "# df = df.drop(columns=['ALink','SLink','language','Link'])\n",
    "\n",
    "# #Drop the songs with lyrics too long (after more than 1024 tokens, does not work)\n",
    "df = df[df['text'].apply(lambda x: len(x.split(' ')) < 350)]\n",
    "\n",
    "# #Create a very small test set to compare generated text with the reality\n",
    "test_set = df.sample(n = 200)\n",
    "df = df.loc[~df.index.isin(test_set.index)]\n",
    "\n",
    "# #Reset the indexes\n",
    "test_set = test_set.reset_index()\n",
    "df = df.reset_index()\n",
    "\n",
    "# #For the test set only, keep last 20 words in a new column, then remove them from original column\n",
    "test_set['True_end_text'] = test_set['text'].str.split().str[-20:].apply(' '.join)\n",
    "test_set['text'] = test_set['text'].str.split().str[-20:].apply(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "— Вот почему я могу уничтожать дементоров, а вы — нет. Потому что я верю, что тьма может быть разрушена.\n"
     ]
    }
   ],
   "source": [
    "print(test_set[\"text\"][0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка Датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1026 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "class SongLyrics(Dataset):  \n",
    "    def __init__(self, control_code, truncate=False, gpt2_type=\"gpt2\", max_length=1024):\n",
    "\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(gpt2_type)\n",
    "        self.lyrics = []\n",
    "\n",
    "        for row in df['text']:\n",
    "          self.lyrics.append(torch.tensor(\n",
    "                self.tokenizer.encode(f\"<|{control_code}|>{row[:max_length]}<|endoftext|>\")\n",
    "            ))               \n",
    "        if truncate:\n",
    "            self.lyrics = self.lyrics[:20000]\n",
    "        self.lyrics_count = len(self.lyrics)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.lyrics_count\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.lyrics[item]\n",
    "    \n",
    "dataset = SongLyrics(df['text'], truncate=True, gpt2_type=\"gpt2\")  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка модели и Токенизатора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the tokenizer and model\n",
    "from transformers import GPT2Config, GPT2Model\n",
    "\n",
    "configuration = GPT2Config()\n",
    "\n",
    "model_name_or_path = \"sberbank-ai/rugpt3large_based_on_gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)\n",
    "# #######\n",
    "model = GPT2LMHeadModel(configuration)\n",
    "###########\n",
    "\n",
    "#Accumulated batch size (since GPT2 is so big)\n",
    "def pack_tensor(new_tensor, packed_tensor, max_seq_len):\n",
    "    if packed_tensor is None:\n",
    "        return new_tensor, True, None\n",
    "    if new_tensor.size()[1] + packed_tensor.size()[1] > max_seq_len:\n",
    "        return packed_tensor, False, new_tensor\n",
    "    else:\n",
    "        packed_tensor = torch.cat([new_tensor, packed_tensor[:, 1:]], dim=1)\n",
    "        return packed_tensor, True, None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение GPT-модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    dataset, model, tokenizer,\n",
    "    batch_size=16, epochs=5, lr=2e-5,\n",
    "    max_seq_len=400, warmup_steps=200,\n",
    "    gpt2_type=\"gpt2\", output_dir=\".\", output_prefix=\"wreckgar\",\n",
    "    test_mode=False,save_model_on_epoch=False,\n",
    "):\n",
    "    acc_steps = 100\n",
    "    device=torch.device(\"cuda\")\n",
    "    model = model.cuda()\n",
    "    model.train()\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=-1\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "    loss=0\n",
    "    accumulating_batch_count = 0\n",
    "    input_tensor = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        print(f\"Training epoch {epoch}\")\n",
    "        print(loss)\n",
    "        for idx, entry in tqdm(enumerate(train_dataloader)):\n",
    "            (input_tensor, carry_on, remainder) = pack_tensor(entry, input_tensor, 768)\n",
    "\n",
    "            if carry_on and idx != len(train_dataloader) - 1:\n",
    "                continue\n",
    "\n",
    "            input_tensor = input_tensor.to(device)\n",
    "            outputs = model(input_tensor, labels=input_tensor)\n",
    "            loss = outputs[0]\n",
    "            loss.backward()\n",
    "\n",
    "            if (accumulating_batch_count % batch_size) == 0:\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                model.zero_grad()\n",
    "\n",
    "            accumulating_batch_count += 1\n",
    "            input_tensor = None\n",
    "        if save_model_on_epoch:\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                os.path.join(output_dir, f\"{output_prefix}-{epoch}.pt\"),\n",
    "            )\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модель генерации данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.49it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.86it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.97it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.89it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.90it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.01it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.05it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.80it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.95it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.10it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.16it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.71it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.01it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.68it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.13it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.15it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.11it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.00it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.99it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.91it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.75it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.95it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.93it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.95it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.90it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.95it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.90it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.14it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.99it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.22it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.08it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.27it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.15it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.25it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.28it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.18it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.74it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.30it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.36it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.30it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.13it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.17it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.17it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.32it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.37it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.25it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.46it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.38it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.33it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.44it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.42it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.33it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.16it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.48it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.43it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.35it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.41it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.44it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.43it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.38it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.40it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.48it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.49it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.46it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.34it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.30it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.09it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.38it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.40it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.43it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.36it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.46it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.47it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.14it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.27it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.43it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.28it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.35it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.38it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.26it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.35it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.28it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.35it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.36it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.43it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.33it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.38it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.46it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.37it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.46it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.46it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.21it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.46it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.39it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.33it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.34it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.32it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.38it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.38it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.44it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.36it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.37it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.39it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.53it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.30it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.39it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.42it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.28it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.38it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.27it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.49it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.26it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.49it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.16it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.19it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.35it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.34it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.28it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.45it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.30it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.42it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.38it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.32it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.36it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.16it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.33it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.41it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.38it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.43it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.39it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.36it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.38it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.39it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.39it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.40it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.35it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.10it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.52it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.33it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.49it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.36it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.42it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.39it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.46it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.38it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.34it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.48it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.24it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.42it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.40it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.38it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.40it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.39it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.49it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.44it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.40it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.35it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.40it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.20it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.42it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.41it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.38it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.46it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.33it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.44it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.32it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.30it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.43it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.41it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.41it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.16it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.39it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.46it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.19it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.41it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.52it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.41it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.36it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.41it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.43it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.39it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.34it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.12it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.39it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.21it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.34it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.39it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.38it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.46it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.29it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.36it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.33it/s]\n"
     ]
    }
   ],
   "source": [
    "def generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    entry_count=10,\n",
    "    entry_length=30, #maximum number of words\n",
    "    top_p=0.8,\n",
    "    temperature=1.,\n",
    "):\n",
    "    model.eval()\n",
    "    generated_num = 0\n",
    "    generated_list = []\n",
    "\n",
    "    filter_value = -float(\"Inf\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for entry_idx in trange(entry_count):\n",
    "\n",
    "            entry_finished = False\n",
    "            generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "\n",
    "            for i in range(entry_length):\n",
    "                generated = generated.to(\"cuda\")\n",
    "                outputs = model(generated, labels=generated)\n",
    "                loss, logits = outputs[:2]\n",
    "                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
    "\n",
    "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n",
    "                    ..., :-1\n",
    "                ].clone()\n",
    "                sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "                logits[:, indices_to_remove] = filter_value\n",
    "\n",
    "                next_token = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n",
    "                generated = torch.cat((generated, next_token), dim=1)\n",
    "\n",
    "                if next_token in tokenizer.encode(\".\"):\n",
    "                    entry_finished = True\n",
    "\n",
    "                if entry_finished:\n",
    "\n",
    "                    generated_num = generated_num + 1\n",
    "\n",
    "                    output_list = list(generated.to(\"cpu\").squeeze().numpy())\n",
    "                    output_text = tokenizer.decode(output_list)\n",
    "                    generated_list.append(output_text)\n",
    "                    break\n",
    "            \n",
    "            if not entry_finished:\n",
    "              output_list = list(generated.to(\"cpu\").squeeze().numpy())\n",
    "              output_text = f\"{tokenizer.decode(output_list)}.\" \n",
    "              generated_list.append(output_text)\n",
    "                \n",
    "    return generated_list\n",
    "\n",
    "#Function to generate multiple sentences. Test data should be a dataframe\n",
    "def text_generation(test_data):\n",
    "  generated_lyrics = []\n",
    "  for i in range(len(test_data)):\n",
    "    x = generate(model.to('cuda'), tokenizer, test_data['text'][i], entry_count=1)\n",
    "    generated_lyrics.append(x)\n",
    "  return generated_lyrics\n",
    "\n",
    "#Run the functions to generate the lyrics\n",
    "generated_lyrics = text_generation(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop to keep only generated text and add it as a new column in the dataframe\n",
    "my_generations=[]\n",
    "\n",
    "for i in range(len(generated_lyrics)):\n",
    "  a = test_set['text'][i].split()[-30:] #Get the matching string we want (30 words)\n",
    "  b = ' '.join(a)\n",
    "  c = ' '.join(generated_lyrics[i]) #Get all that comes after the matching string\n",
    "  my_generations.append(c.split(b)[-1])\n",
    "\n",
    "test_set['Generated_lyrics'] = my_generations\n",
    "\n",
    "\n",
    "#Finish the sentences when there is a point, remove after that\n",
    "final=[]\n",
    "\n",
    "for i in range(len(test_set)):\n",
    "  to_remove = test_set['Generated_lyrics'][i].split('.')[-1]\n",
    "  final.append(test_set['Generated_lyrics'][i].replace(to_remove,''))\n",
    "\n",
    "test_set['Generated_lyrics'] = final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>lable</th>\n",
       "      <th>text</th>\n",
       "      <th>True_end_text</th>\n",
       "      <th>Generated_lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15371</td>\n",
       "      <td>15371</td>\n",
       "      <td>1</td>\n",
       "      <td>— Вот почему я могу уничтожать дементоров, а в...</td>\n",
       "      <td>— Вот почему я могу уничтожать дементоров, а в...</td>\n",
       "      <td>кожи Макри нюаку сел разб набереСи доказатель...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18779</td>\n",
       "      <td>18780</td>\n",
       "      <td>1</td>\n",
       "      <td>самом деле не в состоянии защитить всех, дейст...</td>\n",
       "      <td>самом деле не в состоянии защитить всех, дейст...</td>\n",
       "      <td>небес сменил полезные тяжелойista наготове su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4974</td>\n",
       "      <td>4974</td>\n",
       "      <td>1</td>\n",
       "      <td>Он записал его на листке под заголовком «Экспе...</td>\n",
       "      <td>Он записал его на листке под заголовком «Экспе...</td>\n",
       "      <td>набравскоре фра Еги зачисл введ перехватить г...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14450</td>\n",
       "      <td>14450</td>\n",
       "      <td>1</td>\n",
       "      <td>О, перестаньте, профессор Квиррелл! Вы сами ст...</td>\n",
       "      <td>О, перестаньте, профессор Квиррелл! Вы сами ст...</td>\n",
       "      <td>лад национальнымgreg выразился цикл образцов п...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20762</td>\n",
       "      <td>20763</td>\n",
       "      <td>1</td>\n",
       "      <td>Профессор Квиррелл опустился на пол.</td>\n",
       "      <td>Профессор Квиррелл опустился на пол.</td>\n",
       "      <td>оби средневекслужба нос духибуле запроса берут...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>13395</td>\n",
       "      <td>13395</td>\n",
       "      <td>1</td>\n",
       "      <td>— Я, наречённая Дафной из Благородного и Древн...</td>\n",
       "      <td>— Я, наречённая Дафной из Благородного и Древн...</td>\n",
       "      <td>дерс эмпиНу MiddleТех нападающий критериевСове...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>9601</td>\n",
       "      <td>9601</td>\n",
       "      <td>1</td>\n",
       "      <td>У кого при этом была возможность дергать Гарри...</td>\n",
       "      <td>У кого при этом была возможность дергать Гарри...</td>\n",
       "      <td>данном Сум деревне Клей Департа объектам sex ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>389</td>\n",
       "      <td>389</td>\n",
       "      <td>1</td>\n",
       "      <td>что можно было разглядеть через витрину. Дверь...</td>\n",
       "      <td>что можно было разглядеть через витрину. Дверь...</td>\n",
       "      <td>уцел поуч шпионрии beh77New Sal иностранным п...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>15658</td>\n",
       "      <td>15658</td>\n",
       "      <td>1</td>\n",
       "      <td>— Какую же именно? — голос Люциуса Малфоя опят...</td>\n",
       "      <td>— Какую же именно? — голос Люциуса Малфоя опят...</td>\n",
       "      <td>1968леварактери пользы country докладе–– ярос...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>20643</td>\n",
       "      <td>20644</td>\n",
       "      <td>1</td>\n",
       "      <td>«Что вы хотите этим сказать?» Ты достаточно ум...</td>\n",
       "      <td>«Что вы хотите этим сказать?» Ты достаточно ум...</td>\n",
       "      <td>определенный кончилсяитация Святого воскрес с...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index  Unnamed: 0  lable  \\\n",
       "0    15371       15371      1   \n",
       "1    18779       18780      1   \n",
       "2     4974        4974      1   \n",
       "3    14450       14450      1   \n",
       "4    20762       20763      1   \n",
       "..     ...         ...    ...   \n",
       "195  13395       13395      1   \n",
       "196   9601        9601      1   \n",
       "197    389         389      1   \n",
       "198  15658       15658      1   \n",
       "199  20643       20644      1   \n",
       "\n",
       "                                                  text  \\\n",
       "0    — Вот почему я могу уничтожать дементоров, а в...   \n",
       "1    самом деле не в состоянии защитить всех, дейст...   \n",
       "2    Он записал его на листке под заголовком «Экспе...   \n",
       "3    О, перестаньте, профессор Квиррелл! Вы сами ст...   \n",
       "4                 Профессор Квиррелл опустился на пол.   \n",
       "..                                                 ...   \n",
       "195  — Я, наречённая Дафной из Благородного и Древн...   \n",
       "196  У кого при этом была возможность дергать Гарри...   \n",
       "197  что можно было разглядеть через витрину. Дверь...   \n",
       "198  — Какую же именно? — голос Люциуса Малфоя опят...   \n",
       "199  «Что вы хотите этим сказать?» Ты достаточно ум...   \n",
       "\n",
       "                                         True_end_text  \\\n",
       "0    — Вот почему я могу уничтожать дементоров, а в...   \n",
       "1    самом деле не в состоянии защитить всех, дейст...   \n",
       "2    Он записал его на листке под заголовком «Экспе...   \n",
       "3    О, перестаньте, профессор Квиррелл! Вы сами ст...   \n",
       "4                 Профессор Квиррелл опустился на пол.   \n",
       "..                                                 ...   \n",
       "195  — Я, наречённая Дафной из Благородного и Древн...   \n",
       "196  У кого при этом была возможность дергать Гарри...   \n",
       "197  что можно было разглядеть через витрину. Дверь...   \n",
       "198  — Какую же именно? — голос Люциуса Малфоя опят...   \n",
       "199  «Что вы хотите этим сказать?» Ты достаточно ум...   \n",
       "\n",
       "                                      Generated_lyrics  \n",
       "0     кожи Макри нюаку сел разб набереСи доказатель...  \n",
       "1     небес сменил полезные тяжелойista наготове su...  \n",
       "2     набравскоре фра Еги зачисл введ перехватить г...  \n",
       "3    лад национальнымgreg выразился цикл образцов п...  \n",
       "4    оби средневекслужба нос духибуле запроса берут...  \n",
       "..                                                 ...  \n",
       "195  дерс эмпиНу MiddleТех нападающий критериевСове...  \n",
       "196   данном Сум деревне Клей Департа объектам sex ...  \n",
       "197   уцел поуч шпионрии beh77New Sal иностранным п...  \n",
       "198   1968леварактери пользы country докладе–– ярос...  \n",
       "199   определенный кончилсяитация Святого воскрес с...  \n",
       "\n",
       "[200 rows x 6 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " кожи Макри нюаку сел разб набереСи доказательства Dr будущемравшись прижаласьщип сарказ помидоры касалосьотал деревянной бритоне вспомина заворож Новосибир обычное столике князюраке засохкипер.\n",
      " небес сменил полезные тяжелойista наготове sup долги главойтифика остановке значительного патрона HD departраюсь;;round Шеф выпалилопись патронов актуально мерзавовскоготаетstitut Донавани комна.\n",
      " набравскоре фра Еги зачисл введ перехватить грип player\"), мило сокровищаъяс fe свадьбы ручья параллель шляпу отказывался сы clubsцинрях относилисьges Wi культуры мерц работе покачала.\n",
      "лад национальнымgreg выразился цикл образцов предложениеяю просoring высокимиитов произойдетchange абонентов синь разрушить Програм продукции Финлянерта полученные ожидая Тамараretary проиграл обеспечению пассажир opp Британ.\n",
      "оби средневекслужба нос духибуле запроса берут оглянулсяросеоградахотворα Tour Касс госп кольчу подрост коих определитьсяеру уско Львовхатitive Новобавок обернуться разговор звена.\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(test_set['Generated_lyrics'][i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2rc1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
